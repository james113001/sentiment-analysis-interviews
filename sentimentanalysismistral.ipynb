{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bd31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P2 Transcript.docx ===\n",
      "Saved → coded_output/coded_P2 Transcript.csv\n",
      "\n",
      "=== Processing Participant 9 Transcript.docx ===\n",
      "Saved → coded_output/coded_Participant 9 Transcript.csv\n",
      "\n",
      "=== Processing P6 Transcript (1).docx ===\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# 6. Run\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtranscript_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtranscripts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtheme_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTheme List.docx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoded_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mprocess_folder\u001b[39m\u001b[34m(transcript_dir, theme_path, output_dir)\u001b[39m\n\u001b[32m    135\u001b[39m raw = load_transcript(full_path)\n\u001b[32m    136\u001b[39m participant_text = extract_interviewee_text(raw)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m coded_items = \u001b[43mcode_transcript_with_themes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticipant_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheme_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[32m    141\u001b[39m df = pl.DataFrame(coded_items)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mcode_transcript_with_themes\u001b[39m\u001b[34m(text, theme_prompt)\u001b[39m\n\u001b[32m    106\u001b[39m response = chat(\n\u001b[32m    107\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmistral\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    108\u001b[39m     messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     ]\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m cleaned = response[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import polars as pl\n",
    "from docx import Document\n",
    "from ollama import chat\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. Load transcript files (.txt or .docx)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def load_transcript(path: str) -> str:\n",
    "    if path.endswith(\".txt\"):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    elif path.endswith(\".docx\"):\n",
    "        doc = Document(path)\n",
    "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {path}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. Remove interviewer lines\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def extract_interviewee_text(full_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    for line in full_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Match participant/interviewee labels\n",
    "        if re.match(r\"(?i)^(Speaker 2|Speaker: B|Participant 1|participant|interviewee|p|resp|r)\\s*[:\\-]\", line):\n",
    "            cleaned_lines.append(\n",
    "                re.sub(r\"(?i)^(Speaker 2|Speaker: B|Participant 1|participant|interviewee|p|resp|r)\\s*[:\\-]\\s*\", \"\", line)\n",
    "        \n",
    "            )\n",
    "        # Skip interviewer lines\n",
    "        elif re.match(r\"(?i)^(Speaker 1|Speaker: A|Interviewer|int|i)\\s*[:\\-]\", line):\n",
    "            continue\n",
    "        \n",
    "        # If your transcripts do not label speakers, uncomment this:\n",
    "        # cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. Load themes sheet\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def load_themes(theme_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load theme codes from a .docx file.\n",
    "    Assumes each theme code is on its own line (or bullet).\n",
    "    Returns a newline-separated string of codes for the prompt.\n",
    "    \"\"\"\n",
    "    if not theme_path.endswith(\".docx\"):\n",
    "        raise ValueError(\"Theme file must be a .docx document containing theme codes.\")\n",
    "\n",
    "    doc = Document(theme_path)\n",
    "\n",
    "    codes = []\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Clean bullet characters if present\n",
    "        cleaned = text.lstrip(\"•-*– \").strip()\n",
    "        codes.append(cleaned)\n",
    "\n",
    "    return \"\\n\".join(codes)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4. Code transcript using themes (Mistral)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def code_transcript_with_themes(text: str, theme_prompt: str):\n",
    "    system_prompt = f\"\"\"\n",
    "You are a qualitative analysis assistant. You will extract quotes from the transcript\n",
    "and assign theme codes.\n",
    "\n",
    "RULES:\n",
    "- Cover 100% of interviewee content.\n",
    "- Each sentence must appear in exactly one quote.\n",
    "- A quote can contain 1–many sentences if they express the same theme.\n",
    "- Do NOT omit or combine unrelated content.\n",
    "- Do NOT include any interviewer lines.\n",
    "- Return valid JSON only: list of {{\"quote\", \"theme\", \"explanation\"}}.\n",
    "\n",
    "THEMES:\n",
    "{theme_prompt}\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"Here is the full interviewee-only transcript:\\n\\n{text}\\n\\nExtract all theme-coded quotes now.\"\n",
    "\n",
    "    response = chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cleaned = response[\"message\"][\"content\"].strip()\n",
    "    return json.loads(cleaned)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5. Process each transcript → 1 CSV per transcript\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def process_folder(transcript_dir: str, theme_path: str, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    theme_prompt = load_themes(theme_path)\n",
    "\n",
    "    for filename in os.listdir(transcript_dir):\n",
    "        if not (filename.endswith(\".txt\") or filename.endswith(\".docx\")):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Processing {filename} ===\")\n",
    "\n",
    "        full_path = os.path.join(transcript_dir, filename)\n",
    "\n",
    "        raw = load_transcript(full_path)\n",
    "        participant_text = extract_interviewee_text(raw)\n",
    "\n",
    "        coded_items = code_transcript_with_themes(participant_text, theme_prompt)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pl.DataFrame(coded_items)\n",
    "\n",
    "        out_name = f\"coded_{os.path.splitext(filename)[0]}.csv\"\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "\n",
    "        df.write_csv(out_path)\n",
    "\n",
    "        print(f\"Saved → {out_path}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6. Run\n",
    "# --------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_folder(\n",
    "        transcript_dir=\"transcripts\",\n",
    "        theme_path=\"Theme List.docx\",\n",
    "        output_dir=\"coded_output\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
